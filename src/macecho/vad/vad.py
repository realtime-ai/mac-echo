import collections
from enum import Enum
import numpy as np
import onnxruntime as ort
import os
import urllib.request
from typing import Optional, Tuple, Protocol, runtime_checkable
import logging
import wave

from pydantic import BaseModel, Field, field_validator
from .interface import VADInterface, VadState


logger = logging.getLogger(__name__)


class VadConfig(BaseModel):
    """VADé…ç½®ç±»"""
    threshold: float = Field(default=0.7, ge=0.0, le=1.0,
                             description="VAD æ£€æµ‹é˜ˆå€¼ï¼ŒèŒƒå›´ 0-1")
    sampling_rate: int = Field(default=16000, description="éŸ³é¢‘é‡‡æ ·ç‡")
    padding_duration: float = Field(
        default=0.2, ge=0.0, description="è¯­éŸ³å¼€å§‹å‰çš„å¡«å……æ—¶é•¿ï¼Œå•ä½ç§’")
    min_speech_duration: float = Field(
        default=0.1, ge=0.0, description="æœ€å°è¯­éŸ³æ®µé•¿åº¦ï¼Œå•ä½ç§’")
    silence_duration: float = Field(
        default=0.5, ge=0.0, description="é™éŸ³åˆ¤æ–­æ—¶é•¿ï¼Œå•ä½ç§’")
    per_frame_duration: float = Field(
        default=0.032, gt=0.0, description="æ¯å¸§éŸ³é¢‘çš„æ—¶é•¿ï¼Œå•ä½ç§’ (16kHz: 0.032s=512æ ·æœ¬, 8kHz: 0.032s=256æ ·æœ¬)")
    model_path: str = Field(default="silero_vad.onnx", description="VADæ¨¡å‹è·¯å¾„")

    @field_validator('sampling_rate')
    @classmethod
    def validate_sampling_rate(cls, v):
        # Silero VAD æ”¯æŒçš„é‡‡æ ·ç‡
        if v not in [8000, 16000]:
            raise ValueError(
                'Silero VAD sampling rate must be 8000 or 16000')
        return v


class VadProcessor(VADInterface):
    """æ”¹è¿›çš„VADå¤„ç†å™¨ï¼Œä¿®å¤äº†åŸæœ‰çš„é€»è¾‘é—®é¢˜"""

    def __init__(self, config: Optional[VadConfig] = None, **kwargs):
        """
        åˆå§‹åŒ– VAD å¤„ç†å™¨

        Args:
            config: VadConfigé…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨kwargsåˆ›å»º
            **kwargs: ç›´æ¥ä¼ é€’çš„é…ç½®å‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
        """
        # å¤„ç†é…ç½®
        if config is None:
            # å‘åå…¼å®¹ï¼Œä½¿ç”¨ä¼ ç»Ÿå‚æ•°
            config = VadConfig(**kwargs)
        self.config = config

        # ä¸‹è½½å¹¶åŠ è½½ ONNX æ¨¡å‹
        self._load_model()

        # åˆå§‹åŒ–çŠ¶æ€
        self._reset_internal_state()

        # è®¡ç®—ç›¸å…³å‚æ•°
        self._calculate_parameters()

        logger.info(f"VAD å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"- é‡‡æ ·ç‡: {self.config.sampling_rate}Hz")
        logger.info(f"- æ£€æµ‹é˜ˆå€¼: {self.config.threshold}")
        logger.info(
            f"- æœ€å°è¯­éŸ³æ®µé•¿åº¦: {self.config.min_speech_duration}ç§’ ({self.min_speech_samples}æ ·æœ¬)")
        logger.info(
            f"- é™éŸ³åˆ¤æ–­æ—¶é•¿: {self.config.silence_duration}ç§’ ({self.silence_samples}æ ·æœ¬)")
        logger.info(
            f"- è¯­éŸ³å‰å¡«å……æ—¶é•¿: {self.config.padding_duration}ç§’ ({self.padding_frames}å¸§)")

    def _load_model(self):
        """åŠ è½½VADæ¨¡å‹"""
        try:
            if not os.path.exists(self.config.model_path):
                logger.info(f"ä¸‹è½½ Silero VAD æ¨¡å‹åˆ°: {self.config.model_path}")
                urllib.request.urlretrieve(
                    "https://github.com/snakers4/silero-vad/raw/refs/heads/master/src/silero_vad/data/silero_vad.onnx",
                    self.config.model_path
                )

            # åˆå§‹åŒ– ONNX Runtime
            self.session = ort.InferenceSession(self.config.model_path)

            # æ£€æŸ¥æ¨¡å‹è¾“å…¥è¾“å‡ºè§„æ ¼
            self.input_names = [
                input.name for input in self.session.get_inputs()]
            self.output_names = [
                output.name for output in self.session.get_outputs()]

            logger.info("VAD æ¨¡å‹åŠ è½½æˆåŠŸ")
            logger.info(f"æ¨¡å‹è¾“å…¥: {self.input_names}")
            logger.info(f"æ¨¡å‹è¾“å‡º: {self.output_names}")

            # æ£€æµ‹æ¨¡å‹ç±»å‹
            if 'state' in self.input_names:
                self.model_type = 'new'  # æ–°ç‰ˆæœ¬æ ¼å¼
                logger.info("æ£€æµ‹åˆ°æ–°ç‰ˆæœ¬Silero VADæ¨¡å‹æ ¼å¼")
            elif 'h' in self.input_names and 'c' in self.input_names:
                self.model_type = 'old'  # æ—§ç‰ˆæœ¬æ ¼å¼
                logger.info("æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬Silero VADæ¨¡å‹æ ¼å¼")
            else:
                self.model_type = 'simple'  # ç®€å•æ ¼å¼
                logger.info("æ£€æµ‹åˆ°ç®€å•Silero VADæ¨¡å‹æ ¼å¼")

        except Exception as e:
            logger.error(f"VAD æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def _reset_internal_state(self):
        """é‡ç½®å†…éƒ¨çŠ¶æ€"""
        # ONNXæ¨¡å‹çš„éšè—çŠ¶æ€ - å‚è€ƒsilero.pyçš„å®ç°
        self._state = np.zeros((2, 1, 128), dtype='float32')
        self._context = np.zeros((1, 0), dtype='float32')
        self._last_sr = 0
        self._last_batch_size = 0

        # VADçŠ¶æ€
        self.current_state = VadState.IDLE
        self.is_recording = False

        # éŸ³é¢‘ç¼“å†²
        self.audio_buffer = []
        self.silence_counter = 0

        # å¡«å……ç¼“å†²åŒº - ä¿®å¤ï¼šæ­£ç¡®è®¡ç®—maxlen
        self.padding_buffer = None  # å°†åœ¨_calculate_parametersä¸­åˆå§‹åŒ–

    def _reset_states(self, batch_size=1):
        """é‡ç½®ONNXæ¨¡å‹çŠ¶æ€ - å‚è€ƒsilero.py"""
        self._state = np.zeros((2, batch_size, 128), dtype="float32")
        self._context = np.zeros((batch_size, 0), dtype="float32")
        self._last_sr = 0
        self._last_batch_size = 0

    def _calculate_parameters(self):
        """è®¡ç®—å„ç§å‚æ•°"""
        # æ¯å¸§æ ·æœ¬æ•°
        self.frame_samples = int(
            self.config.per_frame_duration * self.config.sampling_rate)

        # æœ€å°è¯­éŸ³æ ·æœ¬æ•°
        self.min_speech_samples = int(
            self.config.min_speech_duration * self.config.sampling_rate)

        # é™éŸ³æ ·æœ¬æ•°
        self.silence_samples = int(
            self.config.silence_duration * self.config.sampling_rate)

        # å¡«å……å¸§æ•° - ä¿®å¤ï¼šåŸºäºå¸§æ•°è€Œä¸æ˜¯æ ·æœ¬æ•°
        self.padding_frames = int(
            self.config.padding_duration / self.config.per_frame_duration)

        # åˆå§‹åŒ–å¡«å……ç¼“å†²åŒº - ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„maxlen
        if self.padding_frames > 0:
            self.padding_buffer = collections.deque(maxlen=self.padding_frames)
        else:
            self.padding_buffer = collections.deque(maxlen=1)  # è‡³å°‘ä¿æŒ1å¸§

    def _validate_input(self, audio_chunk: np.ndarray) -> np.ndarray:
        """éªŒè¯å’Œæ ‡å‡†åŒ–è¾“å…¥éŸ³é¢‘æ ¼å¼"""
        if audio_chunk is None:
            raise ValueError("éŸ³é¢‘æ•°æ®ä¸èƒ½ä¸ºNone")

        if not isinstance(audio_chunk, np.ndarray):
            audio_chunk = np.array(audio_chunk)

        # è½¬æ¢ä¸ºfloat32
        if audio_chunk.dtype != np.float32:
            if audio_chunk.dtype in [np.int16, np.int32]:
                # æ•´æ•°ç±»å‹éœ€è¦å½’ä¸€åŒ–
                max_val = np.iinfo(audio_chunk.dtype).max
                audio_chunk = audio_chunk.astype(np.float32) / max_val
            else:
                audio_chunk = audio_chunk.astype(np.float32)

        # å¤„ç†å¤šå£°é“éŸ³é¢‘
        if audio_chunk.ndim == 2:
            if audio_chunk.shape[1] > 1:
                # å¤šå£°é“è½¬å•å£°é“
                audio_chunk = np.mean(audio_chunk, axis=1)
            else:
                audio_chunk = audio_chunk.flatten()
        elif audio_chunk.ndim > 2:
            raise ValueError(f"ä¸æ”¯æŒçš„éŸ³é¢‘ç»´åº¦: {audio_chunk.ndim}")

        return audio_chunk

    def is_speech(self, audio_chunk: np.ndarray) -> Tuple[bool, float]:
        """
        æ£€æµ‹éŸ³é¢‘ç‰‡æ®µæ˜¯å¦åŒ…å«è¯­éŸ³

        Returns:
            Tuple[bool, float]: (æ˜¯å¦åŒ…å«è¯­éŸ³, è¯­éŸ³æ¦‚ç‡)
        """
        try:
            audio_chunk = self._validate_input(audio_chunk)

            # ç¡®ä¿éŸ³é¢‘é•¿åº¦æ­£ç¡®
            expected_length = int(
                self.config.per_frame_duration * self.config.sampling_rate)
            if len(audio_chunk) != expected_length:
                # å¡«å……æˆ–æˆªæ–­éŸ³é¢‘
                if len(audio_chunk) < expected_length:
                    audio_chunk = np.pad(
                        audio_chunk, (0, expected_length - len(audio_chunk)))
                else:
                    audio_chunk = audio_chunk[:expected_length]

            # å‚è€ƒsilero.py: ç¡®å®šæ­£ç¡®çš„å¸§å¤§å°
            num_samples = 512 if self.config.sampling_rate == 16000 else 256

            # é‡æ–°è°ƒæ•´éŸ³é¢‘é•¿åº¦ä¸ºæ ‡å‡†å¸§å¤§å°
            if len(audio_chunk) != num_samples:
                if len(audio_chunk) < num_samples:
                    audio_chunk = np.pad(
                        audio_chunk, (0, num_samples - len(audio_chunk)))
                else:
                    audio_chunk = audio_chunk[:num_samples]

            # å‚è€ƒsilero.py: æ·»åŠ æ‰¹æ¬¡ç»´åº¦
            if np.ndim(audio_chunk) == 1:
                audio_chunk = np.expand_dims(audio_chunk, 0)

            batch_size = np.shape(audio_chunk)[0]
            context_size = 64 if self.config.sampling_rate == 16000 else 32

            # å‚è€ƒsilero.py: çŠ¶æ€é‡ç½®é€»è¾‘
            if not self._last_batch_size:
                self._reset_states(batch_size)
            if self._last_sr and self._last_sr != self.config.sampling_rate:
                self._reset_states(batch_size)
            if self._last_batch_size and self._last_batch_size != batch_size:
                self._reset_states(batch_size)

            # å‚è€ƒsilero.py: ä¸Šä¸‹æ–‡ç®¡ç†
            if not np.shape(self._context)[1]:
                self._context = np.zeros(
                    (batch_size, context_size), dtype='float32')

            # å‚è€ƒsilero.py: æ‹¼æ¥ä¸Šä¸‹æ–‡å’Œå½“å‰éŸ³é¢‘
            x = np.concatenate((self._context, audio_chunk), axis=1)

            # å‚è€ƒsilero.py: å‡†å¤‡ONNXè¾“å…¥
            ort_inputs = {
                "input": x,
                "state": self._state,
                "sr": np.array(self.config.sampling_rate, dtype="int64")
            }

            # è¿è¡Œæ¨ç†
            ort_outs = self.session.run(None, ort_inputs)
            out, state = ort_outs

            # æ›´æ–°çŠ¶æ€
            self._state = state
            self._context = x[..., -context_size:]
            self._last_sr = self.config.sampling_rate
            self._last_batch_size = batch_size

            # è·å–è¯­éŸ³æ¦‚ç‡
            speech_prob = float(out[0])

            return speech_prob > self.config.threshold, speech_prob

        except Exception as e:
            logger.error(f"VADæ¨ç†é”™è¯¯: {e}")
            return False, 0.0

    def process_audio(self, audio_chunk: np.ndarray) -> Tuple[Optional[np.ndarray], VadState]:
        """
        å¤„ç†éŸ³é¢‘ç‰‡æ®µï¼Œè¿”å›å®Œæ•´çš„è¯­éŸ³æ®µå’Œå½“å‰çŠ¶æ€

        Args:
            audio_chunk: éŸ³é¢‘æ•°æ®

        Returns:
            Tuple[Optional[np.ndarray], VadState]: (å®Œæ•´è¯­éŸ³æ®µæˆ–None, å½“å‰VADçŠ¶æ€)
        """
        try:
            audio_chunk = self._validate_input(audio_chunk)

            # å°†å½“å‰éŸ³é¢‘ç‰‡æ®µæ·»åŠ åˆ°å¡«å……ç¼“å†²åŒº
            if self.config.padding_duration > 0:
                self.padding_buffer.append(audio_chunk.copy())

            # æ£€æµ‹æ˜¯å¦åŒ…å«è¯­éŸ³
            is_speech_frame, speech_prob = self.is_speech(audio_chunk)

            if is_speech_frame:
                return self._handle_speech_frame(audio_chunk, speech_prob)
            else:
                return self._handle_silence_frame(audio_chunk, speech_prob)

        except Exception as e:
            logger.error(f"éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
            return None, self.current_state

    def _handle_speech_frame(self, audio_chunk: np.ndarray, speech_prob: float) -> Tuple[Optional[np.ndarray], VadState]:
        """å¤„ç†æ£€æµ‹åˆ°è¯­éŸ³çš„å¸§"""
        # é‡ç½®é™éŸ³è®¡æ•°
        self.silence_counter = 0

        if self.current_state == VadState.IDLE:
            # è¯­éŸ³å¼€å§‹
            self.current_state = VadState.SPEECH_START
            self.is_recording = True

            # æ·»åŠ å¡«å……éŸ³é¢‘
            if self.config.padding_duration > 0:
                for padding_chunk in self.padding_buffer:
                    self.audio_buffer.append(padding_chunk.copy())

            logger.debug(f"è¯­éŸ³å¼€å§‹ï¼Œæ¦‚ç‡: {speech_prob:.3f}")

        elif self.current_state in [VadState.SPEECH_START, VadState.SPEECH_CONTINUE]:
            # è¯­éŸ³ç»§ç»­
            self.current_state = VadState.SPEECH_CONTINUE

        # æ·»åŠ å½“å‰éŸ³é¢‘å—
        self.audio_buffer.append(audio_chunk.copy())

        return None, self.current_state

    def _handle_silence_frame(self, audio_chunk: np.ndarray, speech_prob: float) -> Tuple[Optional[np.ndarray], VadState]:
        """å¤„ç†é™éŸ³å¸§"""
        if self.is_recording:
            # å¢åŠ é™éŸ³è®¡æ•°
            self.silence_counter += len(audio_chunk)

            # ç»§ç»­æ·»åŠ éŸ³é¢‘ï¼ˆå¯èƒ½æ˜¯è¯­éŸ³é—´çš„çŸ­æš‚åœé¡¿ï¼‰
            self.audio_buffer.append(audio_chunk.copy())

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é™éŸ³é˜ˆå€¼
            if self.silence_counter >= self.silence_samples:
                return self._finalize_speech_segment()

        return None, self.current_state

    def _finalize_speech_segment(self) -> Tuple[Optional[np.ndarray], VadState]:
        """å®Œæˆè¯­éŸ³æ®µçš„å¤„ç†"""
        self.current_state = VadState.SPEECH_END
        self.is_recording = False

        # æ£€æŸ¥è¯­éŸ³æ®µæ˜¯å¦è¶³å¤Ÿé•¿
        total_samples = sum(len(chunk) for chunk in self.audio_buffer)

        if total_samples >= self.min_speech_samples:
            # è¯­éŸ³æ®µæœ‰æ•ˆï¼Œè¿”å›å®Œæ•´éŸ³é¢‘
            speech_segment = np.concatenate(self.audio_buffer)
            logger.debug(
                f"æ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³æ®µï¼Œé•¿åº¦: {len(speech_segment) / self.config.sampling_rate:.2f}ç§’")

            # é‡ç½®ç¼“å†²åŒº
            self.audio_buffer = []
            self.silence_counter = 0
            self.current_state = VadState.IDLE

            return speech_segment, VadState.SPEECH_END
        else:
            # è¯­éŸ³æ®µå¤ªçŸ­ï¼Œä¸¢å¼ƒ
            logger.debug(
                f"è¯­éŸ³æ®µå¤ªçŸ­ï¼Œä¸¢å¼ƒ: {total_samples / self.config.sampling_rate:.2f}ç§’")
            self.audio_buffer = []
            self.silence_counter = 0
            self.current_state = VadState.IDLE

            return None, VadState.IDLE

    def reset(self):
        """é‡ç½® VAD çŠ¶æ€"""
        logger.debug("é‡ç½®VADçŠ¶æ€")
        self._reset_internal_state()
        self._calculate_parameters()

    def get_stats(self) -> dict:
        """è·å–VADç»Ÿè®¡ä¿¡æ¯"""
        return {
            "current_state": self.current_state.value,
            "is_recording": self.is_recording,
            "buffer_length": len(self.audio_buffer),
            "buffer_duration": sum(len(chunk) for chunk in self.audio_buffer) / self.config.sampling_rate,
            "silence_counter": self.silence_counter,
            "silence_duration": self.silence_counter / self.config.sampling_rate,
            "padding_buffer_size": len(self.padding_buffer) if self.padding_buffer else 0
        }


# å‘åå…¼å®¹çš„å·¥å‚å‡½æ•°
def create_vad_processor(threshold=0.7, sampling_rate=16000, padding_duration=0.2,
                         min_speech_duration=0.1, silence_duration=0.5,
                         per_frame_duration=0.032) -> VadProcessor:
    """åˆ›å»ºVADå¤„ç†å™¨ï¼ˆå‘åå…¼å®¹ï¼‰"""
    config = VadConfig(
        threshold=threshold,
        sampling_rate=sampling_rate,
        padding_duration=padding_duration,
        min_speech_duration=min_speech_duration,
        silence_duration=silence_duration,
        per_frame_duration=per_frame_duration
    )
    return VadProcessor(config)


if __name__ == "__main__":
    # çœŸå®éŸ³é¢‘æ–‡ä»¶æµ‹è¯•
    import logging
    import os
    import wave
    import struct

    logging.basicConfig(level=logging.INFO)

    # æ£€æŸ¥en.wavæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    audio_file_path = os.path.join(os.path.dirname(__file__), "en.wav")
    if not os.path.exists(audio_file_path):
        print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_file_path}")
        print("è¯·ç¡®ä¿en.wavæ–‡ä»¶åœ¨ç›¸åŒç›®å½•ä¸‹")
        exit(1)

    print(f"ğŸ“ åŠ è½½éŸ³é¢‘æ–‡ä»¶: {audio_file_path}")

    # è¯»å–WAVæ–‡ä»¶
    try:
        with wave.open(audio_file_path, 'rb') as wav_file:
            # è·å–éŸ³é¢‘å‚æ•°
            frames = wav_file.getnframes()
            sample_rate = wav_file.getframerate()
            channels = wav_file.getnchannels()
            sample_width = wav_file.getsampwidth()
            duration = frames / sample_rate

            print(f"ğŸµ éŸ³é¢‘ä¿¡æ¯:")
            print(f"  - é‡‡æ ·ç‡: {sample_rate}Hz")
            print(f"  - å£°é“æ•°: {channels}")
            print(f"  - é‡‡æ ·å®½åº¦: {sample_width}å­—èŠ‚")
            print(f"  - æ€»å¸§æ•°: {frames}")
            print(f"  - æ€»æ—¶é•¿: {duration:.2f}ç§’")

            # è¯»å–æ‰€æœ‰éŸ³é¢‘æ•°æ®
            raw_audio = wav_file.readframes(frames)

            if sample_width == 2:
                # 16ä½æœ‰ç¬¦å·
                audio_data = np.frombuffer(raw_audio, dtype=np.int16)
                audio_data = audio_data.astype(np.float32) / 32768.0
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„é‡‡æ ·å®½åº¦: {sample_width}")

            # å¤„ç†å¤šå£°é“éŸ³é¢‘
            if channels > 1:
                raise ValueError(f"ä¸æ”¯æŒçš„å¤šå£°é“éŸ³é¢‘: {channels}")

    except Exception as e:
        print(f"âŒ è¯»å–éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
        exit(1)

    # åˆ›å»ºVADå¤„ç†å™¨
    config = VadConfig(
        threshold=0.5,
        sampling_rate=sample_rate if sample_rate in [8000, 16000] else 16000,
        padding_duration=0.3,
        min_speech_duration=0.1,
        silence_duration=0.8
    )

    vad = VadProcessor(config)
    print(f"âœ… VADå¤„ç†å™¨åˆ›å»ºæˆåŠŸ")

    # åˆ†å¸§å¤„ç†éŸ³é¢‘
    frame_duration = 0.032  # 32mså¸§
    frame_size = int(frame_duration * sample_rate)  # æ¯å¸§æ ·æœ¬æ•°
    total_frames = len(audio_data) // frame_size

    print(f"\nğŸ¬ å¼€å§‹å¤„ç†éŸ³é¢‘:")
    print(f"  - å¸§å¤§å°: {frame_size}æ ·æœ¬ ({frame_duration*1000:.0f}ms)")
    print(f"  - æ€»å¸§æ•°: {total_frames}")
    print(f"  - å¤„ç†æ—¶é•¿: {total_frames * frame_duration:.2f}ç§’")
    print("-" * 50)

    speech_segments = []
    frame_results = []

    for frame_idx in range(total_frames):
        # æå–å½“å‰å¸§
        start_sample = frame_idx * frame_size
        end_sample = start_sample + frame_size
        audio_frame = audio_data[start_sample:end_sample]

        # ç¡®ä¿å¸§é•¿åº¦æ­£ç¡®
        if len(audio_frame) < frame_size:
            audio_frame = np.pad(
                audio_frame, (0, frame_size - len(audio_frame)))

        # VADå¤„ç†
        speech_segment, state = vad.process_audio(audio_frame)
        is_speech, prob = vad.is_speech(audio_frame)

        frame_time = frame_idx * frame_duration
        frame_results.append({
            'frame': frame_idx,
            'time': frame_time,
            'state': state.value,
            'probability': prob,
            'is_speech': is_speech
        })

        if state == VadState.SPEECH_START:
            print(f"ğŸ—£ï¸  å¸§{frame_idx:4d} ({frame_time:5.2f}s): æ£€æµ‹åˆ°è¯­éŸ³æ®µ")

        if state == VadState.SPEECH_END:
            print(f"ğŸ—£ï¸  å¸§{frame_idx:4d} ({frame_time:5.2f}s): æ£€æµ‹åˆ°è¯­éŸ³æ®µç»“æŸ")

        # æ£€æµ‹åˆ°è¯­éŸ³æ®µ
        if speech_segment is not None:
            segment_duration = len(speech_segment) / sample_rate
            speech_segments.append({
                'frame': frame_idx,
                'time': frame_time,
                'duration': segment_duration,
                'samples': len(speech_segment)
            })
            print(
                f"ğŸ—£ï¸  å¸§{frame_idx:4d} ({frame_time:5.2f}s): æ£€æµ‹åˆ°è¯­éŸ³æ®µ {segment_duration:.2f}ç§’")

            # ä¿å­˜è¯­éŸ³æ®µä¸ºWAVæ–‡ä»¶
            output_filename = f"speech_segment_{frame_time:5.2f}.wav"
            with wave.open(output_filename, 'wb') as wav_file:
                wav_file.setnchannels(1)  # å•å£°é“
                wav_file.setsampwidth(2)  # 16ä½
                wav_file.setframerate(sample_rate)
                # å°†float32è½¬æ¢å›int16
                audio_int16 = (speech_segment * 32767).astype(np.int16)
                wav_file.writeframes(audio_int16.tobytes())
            print(f"ğŸ’¾ å·²ä¿å­˜è¯­éŸ³æ®µåˆ°: {output_filename}")

        # æ¯50å¸§æ‰“å°ä¸€æ¬¡çŠ¶æ€
        if frame_idx % 50 == 0:
            stats = vad.get_stats()
            print(f"ğŸ“Š å¸§{frame_idx:4d} ({frame_time:5.2f}s): "
                  f"çŠ¶æ€={state.value:15s}, æ¦‚ç‡={prob:.3f}, "
                  f"ç¼“å†²={stats['buffer_duration']:.2f}s")

    # æœ€ç»ˆç»Ÿè®¡
    print("\n" + "=" * 50)
    print("ğŸ“ˆ å¤„ç†ç»“æœç»Ÿè®¡:")
    print(f"  - æ€»å¤„ç†å¸§æ•°: {len(frame_results)}")
    print(f"  - æ£€æµ‹åˆ°è¯­éŸ³æ®µæ•°: {len(speech_segments)}")

    # è¯­éŸ³/é™éŸ³å¸§ç»Ÿè®¡
    speech_frames = sum(1 for r in frame_results if r['is_speech'])
    silence_frames = len(frame_results) - speech_frames
    speech_duration = speech_frames * frame_duration
    silence_duration = silence_frames * frame_duration

    print(
        f"  - è¯­éŸ³å¸§æ•°: {speech_frames} ({speech_duration:.2f}s, {speech_duration/duration*100:.1f}%)")
    print(
        f"  - é™éŸ³å¸§æ•°: {silence_frames} ({silence_duration:.2f}s, {silence_duration/duration*100:.1f}%)")

    # æ˜¾ç¤ºæ£€æµ‹åˆ°çš„è¯­éŸ³æ®µ
    if speech_segments:
        print(f"\nğŸ¯ æ£€æµ‹åˆ°çš„è¯­éŸ³æ®µ:")
        total_speech_time = sum(seg['duration'] for seg in speech_segments)
        for i, seg in enumerate(speech_segments, 1):
            print(f"  {i}. æ—¶é—´: {seg['time']:.2f}s, æ—¶é•¿: {seg['duration']:.2f}s")
        print(f"  æ€»è¯­éŸ³æ—¶é•¿: {total_speech_time:.2f}s")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°ä»»ä½•è¯­éŸ³æ®µ")

    print(f"\nâœ… éŸ³é¢‘å¤„ç†å®Œæˆï¼")
